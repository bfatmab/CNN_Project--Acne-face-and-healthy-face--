{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3+Q9gqTX7SxeuByp7163U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bfatmab/CNN_Project--Acne-face-and-healthy-face--/blob/main/MobilNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0-V1pf1XnST",
        "outputId": "38026acf-bdeb-4797-d216-c6a10d6ed99e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#Google Drive Baglantisi\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.python import data\n",
        "%cd gdrive/My Drive/CNN_Proje/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hv0zigifDIy",
        "outputId": "10a3f8dd-8363-45b0-bc86-e03db6157dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/CNN_Proje\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation, Dense,Flatten\n",
        "from PIL import Image\n",
        "\n",
        "#Giris olarak verilecek goruntulerin boyutlari\n",
        "imageWidth, imageHeight = 224, 224\n",
        "\n",
        "imageChannels = 3\n",
        "#batchSize = 3 #her seferinde eğitim için alınacak veri miktarı\n",
        "#epoch= 10\n",
        "classMode='binary'\n",
        "\n",
        "\n",
        "\n",
        "#fcDense = 3 # 3 tane class\n",
        "#son katmandaki sınıf sayısı\n",
        "#Binary (iki tane sınıf ) icin: fcDense = 1 olmalı\n",
        "fcActivation= 'softmax'\n",
        "optimizer='adam'\n",
        "loss='sparse_binary_crossentropy' #mse\n",
        "#metrics='accuracy'\n",
        "train_data_yolu='data/train'\n",
        "validation_data_yolu = 'data/validation'\n",
        "train_ornek_sayisi = 1400\n",
        "validation_ornek_sayisi = 600\n",
        "   #train_ornek_sayisi,validation_ornek_sayisi ile batch_size tam bolunmeli\n",
        "epoch = 10\n",
        "batchSize = 100 #her seferinde eğitim için alınacak veri miktarı\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "  input_shape = (3, imageWidth, imageHeight)\n",
        "else :\n",
        "   input_shape = (imageWidth, imageHeight, 3)\n",
        "\n",
        "\n",
        "   #train_ornek_sayisi = 300\n",
        "   #validation_ornek_sayisi = 100\n",
        "   #train_ornek_sayisi,validation_ornek_sayisi ile batch_size tam bolunmeli"
      ],
      "metadata": {
        "id": "ueEJZ4aHfJyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input = Input (input_shape)\n",
        "#Giris katmanı ınput modeli olarak uyarlanıyor\n",
        "model = MobileNet (include_top= False, weights='imagenet', input_tensor= input, classes=3 )\n",
        "# modeling pooling özel. ayarlanabilir: pooling = 'avg'\n",
        "\n",
        "#add new classifier layers\n",
        "flat1 = Flatten() (model.layers[-1].output)\n",
        "class1 = Dense(512 , activation='linear' , name='fc1')(flat1)\n",
        "class2 = Dense(256 , activation='relu' , name='fc2')(class1)\n",
        "output = Dense(1 , activation='softmax' , name='predictions')(class2)\n",
        "\n",
        "#define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "#model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8zAXUgbfOz1",
        "outputId": "9a38ba5a-c66d-4418-c673-42b2a0c047d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam' ,\n",
        "    loss='categorical_crossentropy' ,\n",
        "    metrics=['accuracy'] ,\n",
        "\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "93o1AR3JfTp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#veri hazırlama\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "\n",
        "train_datagen=ImageDataGenerator (preprocessing_function=preprocess_input)\n",
        "validation_datagen=ImageDataGenerator (preprocessing_function=preprocess_input)\n",
        "\n",
        "#validation_split ayarlanarak subset alanlarına bölebilirsinizsubset=\"training\" , subset=\"validation\"\n",
        "#test_datagen=ImageDataGenerator (preprocessing_function=preprocess_input)\n",
        "\n",
        "#\n",
        "#\n",
        "\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "#\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "  train_data_yolu,\n",
        "  target_size=(imageWidth , imageHeight) ,\n",
        "  batch_size=batchSize,\n",
        "  subset=\"training\" ,\n",
        "  class_mode = classMode)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "  validation_data_yolu ,\n",
        "  target_size=(imageWidth , imageHeight) ,\n",
        "  batch_size=batchSize,\n",
        "  class_mode=classMode)\n",
        "\n",
        "  #test_generator = test_datagen.flow_from_directory(\n",
        "  #validation_data_yolu ,\n",
        "  #target_size=(imageWidth , imageHeight) ,\n",
        "  #batch_size=batchSize,\n",
        "  #class_mode=classMode)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpjdI-WzfV7B",
        "outputId": "dfe6d549-857b-4dc3-fa12-22164dc0c8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1400 images belonging to 2 classes.\n",
            "Found 600 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelin eğitimi\n",
        "model.fit(train_generator ,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=validation_ornek_sayisi//batchSize, #test verisi / batchSize\n",
        "          #\n",
        "          steps_per_epoch=train_ornek_sayisi//batchSize ,\n",
        "          #\n",
        "          batch_size=batchSize,\n",
        "          epochs=epoch,\n",
        "          verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shQQ1NHzfYMi",
        "outputId": "437b51bc-3d60-4031-dd6e-00ecc773f94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 503s 35s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 27s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 25s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 24s 2s/step - loss: 0.0000e+00 - accuracy: 0.5000 - val_loss: 0.0000e+00 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fed8a2e81f0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('mobilenet_99val91_model.h5')\n",
        "#model.save(\"VGG16.h5\")\n"
      ],
      "metadata": {
        "id": "cRjRVGAxfbB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "# model.load_weights('VGG16.h5')\n",
        "\n",
        "img = load_img('deneme5.jpg', grayscale=False, color_mode='rgb', target_size=(imageWidth, imageHeight))\n",
        "\n",
        "image = img_to_array(img)  # Numpy kütüphanesi ile bir diziye dönüştürülüyor: shape(3, 224, 224)\n",
        "\n",
        "# Veri modeli için yeniden şekillendirme\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))  # (1, 224, 224, 3)\n",
        "\n",
        "# VGG modeli için görüntüyü hazırlama\n",
        "image = preprocess_input(image)\n",
        "\n",
        "yhat = model.predict(image)\n",
        "\n",
        "print(yhat)\n",
        "\n",
        "\n",
        "\n",
        "# concvert the probabilities to class labels\n",
        "\n",
        "label=decode_predictions(yhat)\n",
        "\n",
        "# retrieve the most likely result ,e.g.highest probaility\n",
        "\n",
        "label=label[0][0]\n",
        "\n",
        "# print the classification\n",
        "\n",
        "print('%s(%.2f%%)'%(label[1],label[2]*100))\n",
        "\n",
        "print (prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rCBanQjfdVu",
        "outputId": "2e239aed-f478-4add-f295-04dfdbfbf043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 430ms/step\n",
            "[[1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUNxYKCOfgDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}